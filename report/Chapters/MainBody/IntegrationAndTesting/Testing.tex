% Testing and integration
% This chapter should describe the testing and integration of the project.
% What is the testing and integration of the project? What is it intended to achieve?
% How should I go about testing and integrating the project?
% What are the testing requirements of the project?
% What did I actually do?
% What did I need to do differently from my initial plan?
\subsection{Testing}

The testing of the project was conducted in three main stages: unit testing; integration testing; and standalone testing. The unit testing was conducted using .NET Unit testing framework, integration testing was conducted using the Visual Studio Integrated Development Environment (IDE), and standalone testing was conducted by running the application in a standalone environment.

\subsubsection{Unit testing}
Unit tests were the primary method of testing the project. Unit tests are particularly useful for an algorithmic system as it allows for the testing of individual components in isolation. In addition to this regression testing, which is the process of testing a program to ensure that changes to one part of the program have not affected other parts of the program, has been used. This testing pattern is particularly useful for this project as it has been build in a modular fashion.

To ensure that the custom regex engine was robust, a comprehensive set of unit tests were written. These tests were designed to test the engine under a variety of conditions, including invalid inputs and a plethora of inputs to be converted to various output formats. The tests that were designed to validate the conversion process used a series of common naming patterns such as camel case, snake case, and pascal case. These tests were as simple as providing the custom engine with an input and comparing the output to an expected result to ensure that the engine was functioning correctly and as expected.
% TODO: Could put these in the appendix. Say "see appendix X".

For testing the code analyzers and fix providers the MSBuild testing framework was used. This framework provides the basic functionality to compile input source files, run the analyzers and fix providers on the compiled source files and then compare the generated diagnostics and fixes to expected results provided by the test.
The work put into making this testing framework more streamlined and easier to use was a significant development in ensuring the reliability of this tool. More detailed information of the workings behind this testing framework can be found in the appendix section \ref{subsec:CustomUnitTestFileParser}.
% This framework requires a custom format for the test data as described by the documentation on the Microsoft website. The format that the framework desires is not ideal as it produces a lot of duplicated code, so to reduce this a custom source file parser was created to drastically cut down the amount of code that was required to write the tests. In this custom source file parser, the source file can contain code and diagnostic comments. The diagnostic comments are formatted as seen in Table \ref{tab:FileParserSyntax}.
% \begin{table}[H]
%     \centering
%     \caption{File Parser Syntax}
%     \label{tab:FileParserSyntax}
%     \begin{tabular}{|p{3.5cm}|p{8cm}|p{4cm}|}
%         % \hline
%         % \multicolumn{3}{|c|}{File Parser Syntax} \\
%         \hline
%         Syntax&Description&Example\\
%         \hline
%         \texttt{//\#\textless Diagnostic ID\textgreater}&Indicates the start of a diagnostic block&\texttt{//\#RFSA0001}\\
%         \texttt{//-\textless Code\textgreater}&Indicates code that should be removed from the source file&\texttt{//- void Test()}\\
%         \texttt{//+\textless Code\textgreater}&Indicates code that should be added to the source file&\texttt{//+ private void Test()}\\
%         \texttt{//''\textless Code\textgreater''}&Indicates the area that the diagnostic should occur&\texttt{//- return 2''+''3; }\\
%         \hline
%     \end{tabular}
% \end{table}

% The file interpreter creates three copies of the source file in memory for the analyzer input, code fix input and expected output. The source file is then parsed for diagnostic comments using a regular expression. Once a diagnostic block is found, each copy of the source file is modified according to the syntax required by the MSBuild testing framework.
% For example, if a single source file had the following content:
% \begin{lstlisting}[style=sharpc]
% using System;
% //#0007
% //- public class ''_class_name_''
% //+ public class ClassName
% {}
% \end{lstlisting}

% The file interpreter will then parse this to produce three files with the required syntax and data for each of MSBuild inputs as well as the diagnostics to be expected from the analyzer. The three files that are produced are as follows:
% % \textbf{Analyzer input}
% % \begin{lstlisting}[style=sharpc]
% % using System;
% % public class _class_name_
% % {}
% % \end{lstlisting}
% % \textbf{Code fix input}
% % \begin{lstlisting}[style=sharpc]
% % using System;
% % public class {{|#0:_class_name_|}}
% % {}
% % \end{lstlisting}
% % \textbf{Expected output}
% % \begin{lstlisting}[style=sharpc]
% % using System;
% % public class ClassName
% % {}
% % \end{lstlisting}
% % \textbf{Expected diagnostics}
% % \texttt{ID: RFSA0007, Message: Class name should match the pattern [A-Z][a-z]+, Severity: Error, Location: Line 2, Column 7}

% \begin{table}[h!]
%     \centering
%     \begin{tabular}{p{9cm}p{8cm}}
%         % \hline
%         \textbf{Expected diagnostic} & \textbf{Analyzer input} \\
%         % \hline
%         \texttt{ID: RFSA0007}\newline
%         \texttt{Message: Class name should match the pattern "[A-Z][a-z]+"}\newline
%         \texttt{Location: Line 2, Column 13, Span: 12}
%         &
%         \begin{lstlisting}[style=sharpc]
% using System;
% public class _class_name_
% {}
%         \end{lstlisting}
%         \\
%         % \hline
%         \textbf{Code fix input} & \textbf{Code fix expected output} \\
%         % \hline
%         \begin{lstlisting}[style=sharpc]
% using System;
% public class {{|#0:_class_name_|}}
% {}
%         \end{lstlisting}
%         &
%         \begin{lstlisting}[style=sharpc]
% using System;
% public class ClassName
% {}
%         \end{lstlisting}
%         \\
%         % \hline
%     \end{tabular}
% \end{table}

% Using this system the test data can be written in a more human-readable format with much less code duplication. This system also allows for the easy addition and modification for new and existing tests.
These unit tests then run the MSBuild diagnostic analyzers and code fix providers with the provided input and expected outputs from the file parser and a reference to the custom modules written for this program. The tests are then run to completion and the results are compared to the expected results. If the tests have a runtime error or the results do not match the expected results the test will fail.
Unit tests also allow for rapid testing and development because unit tests are written to be small and so are often fast to run. The results of these tests can also be easily interpreted as they are either pass or fail and most modern IDEs have built-in integration with unit testing frameworks. This project was developed within Visual Studio Community Edition which has built-in support for the MSTest framework which allowed for the tests to be run and the results to be viewed within the IDE itself with inline hints on where tests may have failed (see figure \ref{fig:UnitTestResults}).

A significant amount of time was spent constantly testing the components of the tool to ensure that they were functioning correctly and as expected. Automated unit test executions were setup so that after each new build of the program, the tests would be re-run. Most of the tests yielded the expected results most of the time, with the exception of a few edge cases that were not major enough to be fixed, in particular any code fix providers that would enter new lines to a document would not always use the correct line ending for the operating system that the tool was running on (see figure \ref{fig:UnitTestFailing}). This was not fixed as it was not a major issue and would not affect the functionality of the tool.

\begin{figure}[H]
    \centering
    \caption{Unit Tests}
    \label{fig:UnitTests}
    \begin{subfigure}[t]{0.35\textwidth}
        \caption{Unit Test Results}
        \label{fig:UnitTestResults}
        \includegraphics[height=0.35\textheight, keepaspectratio]{Figures/IDEUnitTestsCropped2.png}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.35\textwidth}
        \caption{Failing Unit Test}
        \label{fig:UnitTestFailing}
        \includegraphics[width=\linewidth, height=0.35\textheight, keepaspectratio]{Figures/UnitTestFailing.png}
    \end{subfigure}
\end{figure}

Of the automated tests conducted, fifty-eight were created of which fifty-five were successful. The three tests that failed were due to the line ending issue mentioned above. The tests that failed were not fixed as they were not major issues, would not affect the functionality of the tool and also due to a lack of time to resolve the issue. The tests that passed were able to validate the functionality of the tool and ensure that the tool was functioning correctly and as expected. Given the success rate of 94.8\%, it can be concluded that the tool is functioning well within a passable margin of error and is ready enough for real-world use.

\subsubsection{Integration testing}
Using the VSIX build of the project, a plugin compatible with the Visual Studio IDE gets created. This plugin allowed for the tool to be interacted with via IDE hints. In order to examine this a test project was loaded up into Visual Studio with the plugin loaded. After compiling the project the IDE would gather diagnostics from the build tool and display them in the error list and the code editor. It can be seen that amongst the diagnostics generated, the tool has provided hints within the IDE that the user can click on to preview and apply the fix. (see figure \ref{fig:IDEHints}).

\begin{figure}[H]
    % Temporary workaround for fitting the product evaluation onto the page (otherwise a new page with a single line of text is created).
    \vspace{-10px}
    \centering
    \caption{Integrated IDE Hints}
    \label{fig:IDEHints}
    \includegraphics[width=\textwidth]{Figures/IDEHintsCropped.png}
\end{figure}

\subsubsection{Standalone testing}
The standalone package was built to provide a way to run the tool without the need for the Visual Studio IDE. The standalone program is simplistic and takes a C\# solution file from which the analysis can be run on. Any files that can have fixes applied will be displayed in a graphical window with a file differential view. To validate the results of the standalone tool, checks were made to ensure that it doesn't get stuck in a read-eval loop, which did occur during development. In addition to this, the output files were visually inspected to check that the fixes were applied correctly (see figure \ref{fig:StandaloneTool}). Less significant tests were also conducted for the standalone tool which can be read about in the appendix section \ref{sec:StandaloneToolTests}.

\begin{figure}[H]
    \centering
    \caption{Standalone Tool}
    \label{fig:StandaloneTool}
    \includegraphics[width=\textwidth]{Figures/StandaloneToolCropped.png}
\end{figure}
